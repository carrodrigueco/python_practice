GPT-3 es una red neuronal profunda que utiliza el mecanismo de atención para predecir la palabra siguiente en una oración. Se entrena con un corpus de más de 1000 millones de palabras y puede generar texto con una precisión en el nivel de los caracteres. La arquitectura de GPT-3 consta de dos componentes principales: un codificador y un decodificador. El codificador toma como entrada la palabra anterior en la oración y produce una representación de vectores de ella, que, luego, se pasa a través de un mecanismo de atención para producir la predicción de la siguiente palabra. El decodificador toma como entrada la palabra anterior y su representación de vectores, y produce una distribución de probabilidad de todas las palabras posibles dadas dichas entradas.

El rendimiento de GPT-3 está a la par con los mejores modelos de lenguaje para la generación de texto, lo que es significativamente mejor que los modelos de GPT anteriores. El modelo de Turning NLG de Microsoft puede generar texto con precisión en el nivel de los caracteres en un conjunto de pruebas con artículos de Wikipedia, pero requiere una enorme cantidad de datos de entrenamiento para hacerlo. OpenAI afirma que GPT-3 puede alcanzar este nivel de rendimiento sin ningún dato de capacitación adicional después de su período inicial de capacitación previa. Además, GPT-3 es capaz de generar oraciones y párrafos más largos que los modelos anteriores, como BERT de Google y el transformador de NLP de Stanford.